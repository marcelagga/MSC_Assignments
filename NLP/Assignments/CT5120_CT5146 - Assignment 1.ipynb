{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Marcel Aguilar Garcia\n",
    "\n",
    "Student ID: 20235620"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oP1uun77cIh"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
    "\n",
    "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
    "\n",
    "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
    "\n",
    "    My siter|sister go|goes to Tonbury .\n",
    "    \n",
    "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
    "\n",
    "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
    "\n",
    "    My Mum goes out some_times|sometimes .\n",
    "    \n",
    "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token.\n",
    "\n",
    "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIVCSJV-7kDs"
   },
   "source": [
    "## Task 1 (10 Marks)\n",
    "\n",
    "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
    "\n",
    "Then split your data into a test set of 100 lines and a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RznCZ1mw7mfk"
   },
   "outputs": [],
   "source": [
    "lines = open(\"holbrook.txt\").readlines()\n",
    "data = []\n",
    "# Write your code here\n",
    "import nltk\n",
    "\n",
    "for line in lines:\n",
    "    indexes = []\n",
    "    tokenize_line = nltk.word_tokenize(line)\n",
    "    misspelled_line = tokenize_line.copy()\n",
    "    corrected_line  = tokenize_line.copy()\n",
    "    for index,word in enumerate(tokenize_line):\n",
    "        if '|' in word:\n",
    "            misspelled_line[index] = word.split('|')[0]\n",
    "            corrected_line[index]  = word.split('|')[1]    \n",
    "            indexes.append(index)\n",
    "    data.append({'original':misspelled_line,'corrected':corrected_line,'indexes':indexes})\n",
    "\n",
    "# assert(data[2] == {\n",
    "#    'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'], \n",
    "#    'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'], \n",
    "#    'indexes': [9]\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRSX4I0H7pSC"
   },
   "source": [
    "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Kt9aR2Gy7p1C"
   },
   "outputs": [],
   "source": [
    "test = data[:100]\n",
    "train = data[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm5JL7cH7sLK"
   },
   "source": [
    "## **Task 2** (10 Marks): \n",
    "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
    "\n",
    "*Hint: use `Counter` to implement this so it may be called many times*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7ge0uHS-7uEK"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "corrected_sentences = [train[n]['corrected'] for n in range(len(train))]\n",
    "corrected_words = [word.lower() for sentence in corrected_sentences for word in sentence]\n",
    "unique_corrected_words = set(corrected_words)\n",
    "\n",
    "def unigram(word):\n",
    "    # Write your code here.\n",
    "    return Counter(corrected_words)[word] \n",
    "    \n",
    "\n",
    "def prob(word):\n",
    "    # Write your code here.\n",
    "    word = word.lower()\n",
    "    word_counts = unigram(word)\n",
    "    total_number_of_words = len(corrected_words)\n",
    "    return word_counts / total_number_of_words\n",
    "\n",
    "# Test your code with the following\n",
    "# assert(unigram(\"me\")==87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8r8QYj78GPK"
   },
   "source": [
    "## **Task 3** (15 Marks): \n",
    "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 956,
     "status": "ok",
     "timestamp": 1536070558621,
     "user": {
      "displayName": "John McCrae",
      "photoUrl": "//lh3.googleusercontent.com/-whXIBV_wL0Y/AAAAAAAAAAI/AAAAAAAAATE/-2hfaPZsyHM/s50-c-k-no/photo.jpg",
      "userId": "102173405218988557336"
     },
     "user_tz": -60
    },
    "id": "SV9Mu8P38IQE",
    "outputId": "9f29e22b-0f8b-4b92-9d5f-fcde3efec970"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "# Edit distance returns the number of changes to transform one word to another\n",
    "print(edit_distance(\"hello\", \"hi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hm46Lbiz8K8M"
   },
   "source": [
    "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
    "\n",
    "1. Collect the set of all unique tokens in `train`\n",
    "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
    "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
    "\n",
    "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HoilAmFW8PCb"
   },
   "outputs": [],
   "source": [
    "def get_candidates(token):\n",
    "    # Write your code here.\n",
    "    distance_token_to_words = {word:edit_distance(word,token.lower()) for word in unique_corrected_words}\n",
    "    minimum_distance = min(distance_token_to_words.values())\n",
    "    return sorted([word for word, distance in distance_token_to_words.items() \n",
    "                   if distance == minimum_distance], reverse=True)\n",
    "        \n",
    "# Test your code as follows\n",
    "# assert get_candidates(\"minde\") == ['mine', 'mind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGY-eCkN8TIM"
   },
   "source": [
    "## Task 4 (15 Marks):\n",
    "\n",
    "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
    "\n",
    "*Your solution to this should involve `get_candidates`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dIGKE4_P8WGP"
   },
   "outputs": [],
   "source": [
    "def correct(sentence):\n",
    "    # Write your code here\n",
    "    for index,word in enumerate(sentence):\n",
    "        if (word and word.lower()) not in unique_corrected_words:\n",
    "            candidates = {candidate:prob(candidate) for candidate in get_candidates(word)}\n",
    "            best_candidate = max(candidates, key=candidates.get)\n",
    "            sentence[index] = best_candidate\n",
    "    return sentence\n",
    "\n",
    "# assert(correct([\"this\",\"whitr\",\"cat\"]) == ['this','white','cat'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG7jC6au8kka"
   },
   "source": [
    "## **Task 5** (10 Marks): \n",
    "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1536071822989,
     "user": {
      "displayName": "John McCrae",
      "photoUrl": "//lh3.googleusercontent.com/-whXIBV_wL0Y/AAAAAAAAAAI/AAAAAAAAATE/-2hfaPZsyHM/s50-c-k-no/photo.jpg",
      "userId": "102173405218988557336"
     },
     "user_tz": -60
    },
    "id": "HSXTQypR8mdR",
    "outputId": "853813e4-d71b-42a7-8e96-68d038457628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8380281690140845\n"
     ]
    }
   ],
   "source": [
    "def accuracy(test):\n",
    "    # Write your code here\n",
    "    count_total_words = 0\n",
    "    count_corrected_words = 0\n",
    "    for sentence in test:\n",
    "        corrected_sentence = correct(sentence['original'].copy())\n",
    "        count_total_words+=len(sentence['corrected'])\n",
    "        count_corrected_words += sum(corrected_sentence[n] == sentence['corrected'][n] \n",
    "                                     for n in range(len(sentence['corrected'])))\n",
    "    return count_corrected_words/count_total_words\n",
    "\n",
    "print(accuracy(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b-r2JzD8_Zh"
   },
   "source": [
    "## **Task 6 (35 Marks):**\n",
    "\n",
    "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
    "\n",
    "* You may resources beyond those provided here.\n",
    "* You must **not use the test data** in this task.\n",
    "* Provide a short text describing what you intend to do and why. \n",
    "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
    "* Your implementation should not consist of more than 50 lines of code\n",
    "\n",
    "Please note this task is marked according to: demonstration of knowledge from the lectures (10), originality and appropriateness of solution (10), completeness of description (10) and technical correctness (5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Explanation:**\n",
    "* As the train set is relatively small, I have decided to change the unigram probability function to use \"add one smoothing\" which, after testing this in the test set, it gave a significant improvement from the previous algorithm\n",
    "\n",
    "* On the other hand, I have defined all bigrams from the train set and their frequencies in order to calculate their probability\n",
    "\n",
    "* Finally, I have interpolated the unigram probability into the bigram probability in the function interpolation_probability. After trying some values for lambda, I have decided to give 70% of weight to the bigram probability and 30% to the unigram probability.\n",
    "\n",
    "* As the training set is not large, I realised that the previous algorithm was attempting to correct too many proper nouns.  Most of these nouns were not found in the train set and were being corrected to the wrong word. Because of this, I have decided to ignore words that start by capital letter.\n",
    "\n",
    "* Again, as the training set is not large enough, I thought it would be a good idea to limit the distance between words in get_candidates. I decided that words that are \"far\" enough from the candidates, will not be attempted to be corrected. \n",
    "\n",
    "* By doing all these changes, I brought the previous accuracy 83.8% up to 90.14% which is around 6.34% improvement in the corrections\n",
    "\n",
    "* Please, note that in an attempt of limiting the code to 50 lines, I have summarised some functions more than I would like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from collections import Counter\n",
    "corrected_sentences = [train[n]['corrected'] for n in range(len(train))]\n",
    "corrected_words = [word.lower() for sentence in corrected_sentences for word in sentence]\n",
    "unique_corrected_words = set(corrected_words)\n",
    "finder = BigramCollocationFinder.from_words(corrected_words)\n",
    "bigram_freq_dictionary = dict(finder.ngram_fd.items())\n",
    "\n",
    "def prob(word): \n",
    "    return (Counter(corrected_words)[word]+1) / (len(corrected_words)+len(unique_corrected_words))\n",
    "\n",
    "def bigrams_starting_by(word): \n",
    "    return [t for t in list(bigram_freq_dictionary.keys()) if t[0] == word]\n",
    "\n",
    "def return_dictionary_value(bigram):\n",
    "    try:\n",
    "        return bigram_freq_dictionary[bigram]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "def count_bigrams(list_bigrams): \n",
    "    return sum([return_dictionary_value(bigram) for bigram in list_bigrams])\n",
    "\n",
    "def probability_bigram(word1,word2):\n",
    "    if count_bigrams([(word1,word2)]) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return count_bigrams([(word1,word2)])/count_bigrams(bigrams_starting_by(word1))\n",
    "\n",
    "def interpolation_probability(word1,word2,lambda_1 = 0.3): \n",
    "    return (1-lambda_1)*probability_bigram(word1,word2)+lambda_1*prob(word2)\n",
    "    \n",
    "def get_candidates(token):\n",
    "    distance_token_to_words = {word:edit_distance(word,token.lower()) for word in unique_corrected_words}\n",
    "    minimum_distance = min(distance_token_to_words.values())\n",
    "    if minimum_distance < 2:\n",
    "        return sorted([word for word, distance in distance_token_to_words.items() if distance == minimum_distance])\n",
    "    return [token]\n",
    "\n",
    "def correct(sentence):\n",
    "    for index,word in enumerate(sentence):\n",
    "        if ((word and word.lower()) not in unique_corrected_words) and (not word[0].isupper()):\n",
    "            if index == 0: \n",
    "                previous_word = '.'\n",
    "            else:\n",
    "                previous_word = sentence[index-1].lower()\n",
    "            candidates = {candidate:interpolation_probability(previous_word,candidate) for candidate in get_candidates(word)}\n",
    "            sentence[index] = max(candidates, key=candidates.get)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLzaC6D28sK9"
   },
   "source": [
    "## **Task 7 (5 Marks):**\n",
    "\n",
    "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9014084507042254\n"
     ]
    }
   ],
   "source": [
    "def accuracy(test):\n",
    "    # Write your code here\n",
    "    count_total_words = 0\n",
    "    count_corrected_words = 0\n",
    "    for sentence in test:\n",
    "        corrected_sentence = correct(sentence['original'].copy())\n",
    "        count_total_words+=len(sentence['corrected'])\n",
    "        count_corrected_words += sum(corrected_sentence[n] == sentence['corrected'][n] \n",
    "                                     for n in range(len(sentence['corrected'])))\n",
    "    return count_corrected_words/count_total_words\n",
    "\n",
    "print(accuracy(test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CT5120/CT5146 - Assignment 1",
   "provenance": [
    {
     "file_id": "12crGPce24mcgITZPs7hU_9CPnLAcyIq6",
     "timestamp": 1603097790764
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
