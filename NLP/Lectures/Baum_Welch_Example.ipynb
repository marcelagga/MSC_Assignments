{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baum-Welch Example",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrSLP1GirWmX"
      },
      "source": [
        "# Baum-Welch Implementation\n",
        "\n",
        "In this sheet we will go through an implementation of the Baum-Welch algorithm and use it to solve some models.\n",
        "\n",
        "We will start by creating a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEDvHKM1rf6C"
      },
      "source": [
        "N = 5 # Number of states\n",
        "M = 4 # Number of words\n",
        "data = [\n",
        "  \"A A B C\".split(),\n",
        "  \"A B B A A D\".split(),\n",
        "  \"D A B B C\".split(),\n",
        "  \"B D A C A\".split()\n",
        "] # Our corpus\n",
        "T = len(data)\n",
        "\n",
        "words = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n",
        "\n",
        "def word(n,i):\n",
        "  return words[data[n][i-1]]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Giv5_j2Mr-sP"
      },
      "source": [
        "We initialize a model at random. Note that the probabilities must still be valid distributions so we normalize so they sum to one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BENFAMuNsC1S"
      },
      "source": [
        "import numpy as np\n",
        "from random import random\n",
        "\n",
        "A = np.zeros((N,N))\n",
        "B = np.zeros((N,M))\n",
        "\n",
        "# For convenience the start tag is at zero\n",
        "A[(0,0)] = 1\n",
        "\n",
        "for i in range(0,N):\n",
        "  for j in range(1,N):\n",
        "    A[(i,j)] = random()\n",
        "  s = sum(A[(i,)])\n",
        "  for j in range(N):\n",
        "    A[(i,j)] /= s\n",
        "\n",
        "for i in range(1,N):\n",
        "  for j in range(M):\n",
        "    B[(i,j)] = random()\n",
        "  s = sum(B[(i,)])\n",
        "  for j in range(M):\n",
        "    B[(i,j)] /= s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSGJbBkns_Jd"
      },
      "source": [
        "The next step is to implement the forward algorithm to calculate $\\alpha_{s,i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gwExDTEs-VH",
        "outputId": "a7009d77-a69c-4517-8c2a-63cf1078d02c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def forwards():\n",
        "  alphas = [np.zeros((N,len(t)+1)) for t in data]\n",
        "  for n in range(T):\n",
        "    alphas[n][(0,0)] = 1 # Initialize start probability\n",
        "    for i in range(1,len(data[n])+1):\n",
        "      for s in range(N):\n",
        "\n",
        "        alphas[n][(s,i)] = sum(alphas[n][(t,i-1)] * A[(t,s)] * B[(s,word(n,i))] for t in range(N))\n",
        "  return alphas\n",
        "\n",
        "forwards()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00],\n",
              "        [0.00000000e+00, 2.95842141e-02, 4.68160038e-03, 9.40436407e-04,\n",
              "         3.58607447e-04],\n",
              "        [0.00000000e+00, 6.01229351e-03, 1.06792129e-03, 7.18616985e-04,\n",
              "         5.93193274e-04],\n",
              "        [0.00000000e+00, 5.14040257e-02, 3.89700192e-03, 2.78708837e-03,\n",
              "         2.22401179e-04],\n",
              "        [0.00000000e+00, 2.00610402e-02, 1.00146184e-02, 1.89137217e-03,\n",
              "         1.05298614e-03]]),\n",
              " array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "        [0.00000000e+00, 2.95842141e-02, 5.38756829e-03, 1.65330428e-03,\n",
              "         4.41694970e-04, 7.63520835e-05, 2.79737043e-05],\n",
              "        [0.00000000e+00, 6.01229351e-03, 5.12014827e-03, 1.63357595e-03,\n",
              "         1.05430201e-04, 1.40782937e-05, 1.09586743e-05],\n",
              "        [0.00000000e+00, 5.14040257e-02, 1.21230311e-02, 3.71781065e-03,\n",
              "         3.64280049e-04, 8.33069954e-05, 1.24104419e-05],\n",
              "        [0.00000000e+00, 2.00610402e-02, 1.02349158e-02, 3.07965914e-03,\n",
              "         9.22724815e-04, 1.72419623e-04, 1.53131365e-06]]),\n",
              " array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 0.00000000e+00],\n",
              "        [0.00000000e+00, 5.75367821e-02, 5.69445912e-03, 1.06990768e-03,\n",
              "         3.58960561e-04, 1.24910257e-04],\n",
              "        [0.00000000e+00, 2.45705020e-02, 1.37293893e-03, 8.24417998e-04,\n",
              "         3.54662368e-04, 2.07454766e-04],\n",
              "        [0.00000000e+00, 4.11193696e-02, 4.33031966e-03, 3.14069236e-03,\n",
              "         7.98996469e-04, 7.79354880e-05],\n",
              "        [0.00000000e+00, 9.37281711e-04, 1.08710775e-02, 2.13338248e-03,\n",
              "         6.81453728e-04, 3.58451831e-04]]),\n",
              " array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "         0.00000000e+00, 0.00000000e+00],\n",
              "        [0.00000000e+00, 3.40454034e-02, 2.12868190e-02, 1.85795744e-03,\n",
              "         3.87825531e-04, 1.02807704e-04],\n",
              "        [0.00000000e+00, 2.88259391e-02, 1.20563560e-02, 4.43759126e-04,\n",
              "         5.06160517e-04, 2.21526818e-05],\n",
              "        [0.00000000e+00, 1.59910776e-01, 5.59559103e-03, 1.46432233e-03,\n",
              "         3.13278822e-04, 1.00114973e-04],\n",
              "        [0.00000000e+00, 2.05023345e-02, 1.05567907e-03, 3.35479765e-03,\n",
              "         1.19062678e-03, 2.16244216e-04]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxIMrdQHVkxa"
      },
      "source": [
        "Let's quickly verify this implementation works as a language model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4auqv-sVoVI"
      },
      "source": [
        "def lm():\n",
        "  a = forwards()\n",
        "  return np.sum([a[n][:,len(data[n])] for n in range(T)], axis=1)\n",
        "\n",
        "lm_scores_old = lm()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuJOG24hwRSe"
      },
      "source": [
        "Similarly we implement the backwards algorithm to calculate $\\beta_{s,i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx0-DFoiwZeZ",
        "outputId": "5bf15742-890c-439d-a557-66f702d944e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def backwards():\n",
        "  betas = [np.zeros((N, len(t)+1)) for t in data]\n",
        "  for n in range(T):\n",
        "    for s in range(1,N):\n",
        "      betas[n][(s,len(data[n]))] = 1\n",
        "    for i in reversed(range(0, len(data[n]))):\n",
        "      for s in range(N):\n",
        "        betas[n][(s,i)] = sum(betas[n][(t,i+1)] * A[(s,t)] * B[(t,word(n,i+1))] for t in range(N))\n",
        "  return betas\n",
        "\n",
        "backwards()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.00222719, 0.01151216, 0.08821106, 0.17921754, 0.        ],\n",
              "        [0.00405032, 0.02138238, 0.11663247, 0.32226689, 1.        ],\n",
              "        [0.00326397, 0.0175322 , 0.09463925, 0.379882  , 1.        ],\n",
              "        [0.00380577, 0.02036852, 0.09925946, 0.373101  , 1.        ],\n",
              "        [0.00418016, 0.02204145, 0.1191538 , 0.3231841 , 1.        ]]),\n",
              " array([[5.28741342e-05, 3.74400801e-04, 1.25499908e-03, 3.00500595e-03,\n",
              "         1.70544908e-02, 1.24163935e-01, 0.00000000e+00],\n",
              "        [9.78486187e-05, 5.30446645e-04, 1.74680128e-03, 5.47693744e-03,\n",
              "         2.91259911e-02, 1.56406214e-01, 1.00000000e+00],\n",
              "        [7.99125807e-05, 4.27392620e-04, 1.37594168e-03, 4.43145833e-03,\n",
              "         2.41276047e-02, 1.95904081e-01, 1.00000000e+00],\n",
              "        [9.29463922e-05, 4.61354569e-04, 1.49171447e-03, 5.15883463e-03,\n",
              "         2.72221365e-02, 1.63215715e-01, 1.00000000e+00],\n",
              "        [1.00895565e-04, 5.43152052e-04, 1.79131877e-03, 5.65010453e-03,\n",
              "         2.98562157e-02, 1.42542870e-01, 1.00000000e+00]]),\n",
              " array([[7.68752343e-04, 3.53578705e-03, 2.50144537e-02, 8.82110601e-02,\n",
              "         1.79217543e-01, 0.00000000e+00],\n",
              "        [9.79069965e-04, 6.53596200e-03, 3.54047583e-02, 1.16632470e-01,\n",
              "         3.22266888e-01, 1.00000000e+00],\n",
              "        [1.18238554e-03, 5.33805994e-03, 2.87460124e-02, 9.46392503e-02,\n",
              "         3.79881999e-01, 1.00000000e+00],\n",
              "        [9.97975761e-04, 6.20677769e-03, 3.09074496e-02, 9.92594639e-02,\n",
              "         3.73100996e-01, 1.00000000e+00],\n",
              "        [8.94001709e-04, 6.73919330e-03, 3.62278476e-02, 1.19153799e-01,\n",
              "         3.23184102e-01, 1.00000000e+00]]),\n",
              " array([[4.41319575e-04, 1.37830138e-03, 6.84329246e-03, 3.17669366e-02,\n",
              "         1.07061574e-01, 0.00000000e+00],\n",
              "        [5.85372193e-04, 1.75942038e-03, 1.18488470e-02, 5.96098619e-02,\n",
              "         1.90254928e-01, 1.00000000e+00],\n",
              "        [4.99265251e-04, 2.12790479e-03, 9.52299057e-03, 6.65555341e-02,\n",
              "         1.54750511e-01, 1.00000000e+00],\n",
              "        [5.12116295e-04, 1.79580803e-03, 1.09711138e-02, 6.75187055e-02,\n",
              "         1.78411621e-01, 1.00000000e+00],\n",
              "        [5.94691665e-04, 1.60525560e-03, 1.22128799e-02, 6.02609265e-02,\n",
              "         1.95957859e-01, 1.00000000e+00]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKNgYLIs3oAG"
      },
      "source": [
        "Let's consider these two variables we have calculated. $\\alpha_{s,i}$ represents **the probability of all subsequences from $1,\\ldots,i$ where the $i^{th}$ tag is $s$**.\n",
        "\n",
        "Similarly $\\beta_{s,i}$ represents **the probability of all sequences for $i,\\ldots,n$ where the $i^{th}$ tag is $s$**\n",
        "\n",
        "It then follows that if we multiply these values we get **$\\alpha_{s,i}\\beta_{s,i}$ the probability of all sequnces where the $i^{th}$ tag is $s$**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuOU8Thd4S_3",
        "outputId": "8d1a350f-0c63-4031-9660-70dfa7276423",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "alpha = forwards()\n",
        "beta = backwards()\n",
        "\n",
        "[alpha[0][(s,2)] * beta[0][(s,2)] for s in range(N)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0,\n",
              " 0.0005460266143265999,\n",
              " 0.00010106727064248265,\n",
              " 0.00038681432141839556,\n",
              " 0.0011932798305450373]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqA2sE0T5LnA"
      },
      "source": [
        "If we normalize this, we get **the probability that the $i^{th}$ tag is $s$**. We call this $\\gamma_{i}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIZxgIId5Zj1",
        "outputId": "98376590-7ff7-42c0-f5c5-e8839a8bd247",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def gamma(n,i,alpha,beta):\n",
        "  s = np.array([alpha[n][(s,i)] * beta[n][(s,i)] for s in range(N)])\n",
        "  return s / sum(s)\n",
        "\n",
        "print(gamma(2,0,alpha,beta))\n",
        "print(gamma(2,1,alpha,beta))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1. 0. 0. 0. 0.]\n",
            "[0.         0.48917994 0.17061257 0.33199091 0.00821659]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwP8hMnJ5_l3"
      },
      "source": [
        "If we wish we can know figure out how many times a particular tag is expected to be used in the corpus. Remember that \n",
        "\n",
        "$E[c(x)] = \\sum p(x)$\n",
        "\n",
        "Hence we have that\n",
        "\n",
        "$E[c(t)] = \\sum_i \\gamma_i(t)$\n",
        "\n",
        "However, for the hidden Markov model we actually wish to find the expected count per output word, $k$. We do this by only summing the probabilities for positions where we have word $k$, e.g.,\n",
        "\n",
        "$E[c(k,t)] = \\sum_{i | w_i = k} \\gamma_i(t)$\n",
        "\n",
        "We can use this to calculate our new prediction for $b$\n",
        "\n",
        "$b^*_{t,k} = \\frac{\\sum_{i|w_i = k} \\gamma_i(t)}{\\sum_i \\gamma_i(t)}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv49FvrO7CGF",
        "outputId": "f5035b1b-9aea-4ff2-9fad-a66e1bb3b8c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def b_star(alpha,beta):\n",
        "  b_star = np.zeros((N,M))\n",
        "  s = np.zeros((N))\n",
        "  for n in range(T):\n",
        "    for i in range(1,len(data[n])+1):\n",
        "      g = gamma(n,i,alpha,beta)\n",
        "      b_star[:,word(n,i)] += g\n",
        "      s += g\n",
        "  for i in range(1,N):\n",
        "    for j in range(M):\n",
        "      b_star[(i,j)] /= s[i]\n",
        "  return b_star\n",
        "\n",
        "b_star(alpha,beta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.40377193, 0.18471569, 0.0970583 , 0.31445408],\n",
              "       [0.15953337, 0.31455106, 0.27767608, 0.24823949],\n",
              "       [0.37203064, 0.45009411, 0.05642139, 0.12145386],\n",
              "       [0.51603654, 0.25020163, 0.22364532, 0.01011651]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4JCG58xO--N"
      },
      "source": [
        "For the transition probabilities we need to know how often we see a tag $s$ followed by a tag $t$, we can use the probability $\\alpha_{s,i}$ and the probability $\\beta_{t,i+1}$, which including the relevant probabilities for word $i+1$ is:\n",
        "\n",
        "$\\alpha_{s,i}\\beta_{t,i+1}a_{s,t}b_{t,w_{i+1}}$\n",
        "\n",
        "We normalize this to get the probability $\\xi_i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO7bdA7oPxg_",
        "outputId": "4f45c67a-6b66-4949-aa72-285c79734ff9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def xi(n,i,alpha,beta):\n",
        "  s = np.array([[alpha[n][s,i] * beta[n][t,i+1] * A[s,t] * B[t,word(n,i)] for t in range(N)] for s in range(N)])\n",
        "  return s / sum(sum(s))\n",
        "\n",
        "print(xi(0,0,alpha,beta))\n",
        "print(xi(0,2,alpha,beta))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.22894112 0.26627852 0.28517903 0.21960133]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.        ]]\n",
            "[[0.         0.         0.         0.         0.        ]\n",
            " [0.         0.05305068 0.00884204 0.07173576 0.10693426]\n",
            " [0.         0.01434789 0.00626464 0.0048129  0.01874946]\n",
            " [0.         0.04489464 0.01536729 0.02673438 0.0981234 ]\n",
            " [0.         0.09794419 0.01497941 0.16356243 0.25365664]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4urdIpqgRPTt"
      },
      "source": [
        "As before we use the expected counts in order to estimate our next probability function:\n",
        "\n",
        "$a^*_{s,t} = \\frac{E(c(s,t))}{E(c(s))} = \\frac{\\sum_i\\xi_i(s,t)}{\\sum_i\\gamma_i(s)}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq2J9StzRqbM"
      },
      "source": [
        "def a_star(alpha, beta):\n",
        "  a_star = np.zeros((N,N))\n",
        "  s = np.zeros(N)\n",
        "  for n in range(len(data)):\n",
        "    for i in range(len(data[n])):\n",
        "      x = xi(n,i,alpha,beta)\n",
        "      a_star += x\n",
        "      s += np.sum(x,axis=1)\n",
        "  for i in range(N):\n",
        "    for j in range(1,N):\n",
        "      a_star[(i,j)] /= s[i]\n",
        "  return a_star\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIq9mJopSOPR"
      },
      "source": [
        "We can now define the Baum-Welch algorithm. For this simple version we will just run it for 5 iterations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_P2ZMB5SUbx"
      },
      "source": [
        "def baum_welch():\n",
        "  global A,B\n",
        "  for _ in range(5):\n",
        "    alpha = forwards()\n",
        "    beta = backwards()\n",
        "    a_st = a_star(alpha,beta)\n",
        "    b_st = b_star(alpha,beta)\n",
        "    A = a_st\n",
        "    B = b_st\n",
        "\n",
        "baum_welch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knpdnLHyWsaM"
      },
      "source": [
        "We can verify that this has improved the model by looking at the language model scores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN-FhnhLWwjj",
        "outputId": "5f6dfaed-e31c-46c6-f155-dbf40c627b54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(lm() / lm_scores_old)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 3.64809603 14.12747231  1.55176063  1.97437677]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTki6-OZa7d5"
      },
      "source": [
        "As all (or nearly all) of the values are greater than 1 in the above we can see that the model now thinks the current data is more likely, showing that the model has fitted to the current data. ðŸŽ‰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p929JISRbI2B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}